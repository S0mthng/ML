{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Скачайте датасет MNIST.\n",
        "2. Изучите выборку (какие там картинки, сколько картинок в каждом классе).\n",
        "3. Реализуйте загрузку датасета в память с диска с помощью OpenCV. Использовать готовые реализации датасетов и даталоудеров торча запрещено. Пока можно не заморачиваться многопоточностью.\n",
        "4. Реализуйте аугментацию данных: поворот картинки на определенное кол-во градусов (задается аргументом), зашумление картинки. Реализовывать на numpy. Использовать готовые решения тоже запрещено.\n",
        "5. Реализуйте способ скормить это нейросети (нормализация, приведение к одному разрешению).\n",
        "6. Напишите код нейросети. Использовать Sequential запрещено. Модель пишете руками, наследуясь от torch.nn.Module.\n",
        "7. Реализуйте цикл обучения нейросети. Использовать готовые трейнеры и model.fit() запрещено. Кол-во эпох, learning rate и прочие параметры задаются аргументами.\n",
        "8. Напишите код для инференса и оценки качества работы нейронной сети.\n",
        "9. Оценка качества должна заключаться в подсчете Accuracy, Recall, Precision, F-мера. Пользоваться готовыми решениями тоже запрещено.\n",
        "10. Оформите это как удобную библиотечку, которую можно склонить с гитхаба, положить куда надо файлы весов модели, выборку и обучить / инференсить модель."
      ],
      "metadata": {
        "id": "TR47YfD1uZM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import gzip\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimize\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-z7HCFXFZCeY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./MNIST/archive/\"\n",
        "os.makedirs(path, exist_ok = True)\n",
        "#learning_rate = 0.005\n",
        "epochs = 10\n",
        "batch_size = 256\n",
        "seed = 123456\n",
        "gen = np.random.default_rng(seed)\n",
        "\n",
        "source = {\"training_images\" : \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
        "          \"training_labels\" : \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
        "          \"test_images\" : \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
        "          \"test_labels\" : \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
        "          }"
      ],
      "metadata": {
        "id": "mFVsWvRyDacz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  data={}\n",
        "\n",
        "  for key in source:\n",
        "    data[key] = source[key].split('/')[-1]\n",
        "    if os.path.exists(path + data[key]):\n",
        "      print(data[key] + \" was here long before you were born...\")\n",
        "    else:\n",
        "      urllib.request.urlretrieve(source[key], path + data[key])\n",
        "      print(data[key] + \" downloaded right now!\")\n",
        "\n",
        "  images={}\n",
        "  labels={}\n",
        "\n",
        "  # Images + Normalization\n",
        "  for key in (\"training_images\", \"test_images\"):\n",
        "    with gzip.open(os.path.join(path, data[key]), \"rb\") as mnist:\n",
        "      images[key] = np.frombuffer(mnist.read(), np.uint8, offset = 16).reshape(-1, 28 * 28) / 255\n",
        "      print(\"The shape of {}: {}\".format(key, images.shape))\n",
        "\n",
        "  # Labels\n",
        "  for key in (\"training_labels\", \"test_labels\"):\n",
        "    with gzip.open(os.path.join(path, data[key]), \"rb\") as mnist:\n",
        "      labels[key] = np.frombuffer(mnist.read(), np.uint8, offset = 8)\n",
        "      print(\"The shape of {}: {}\".format(key, labels.shape))\n",
        "\n",
        "  #Permutation\n",
        "  permutation = np.random.permutation(len(labels[key]))\n",
        "\n",
        "  print(images[\"training_images\"][0])\n",
        "  print(labels[\"training_labels\"][0])\n",
        "\n",
        "  images[\"training_images\"] = images[permutation]\n",
        "  labels[\"training_labels\"] = labels[permutation]\n",
        "  print(permutation)\n",
        "\n",
        "  print(images[\"training_images\"][0])\n",
        "  print(labels[\"training_labels\"][0])\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def noise(image):\n",
        "  noise = np.random.normal(0.48, 0.05, image.shape)\n",
        "  gaussedImage = np.clip(image + noise, 0, 1)\n",
        "  return gaussedImage\n",
        "\n",
        "def rotate(image):\n",
        "  angle = np.random.randint(-30, 30)\n",
        "  theta = np.radians(angle)\n",
        "  cos, sin = np.cos(theta), np.sin(theta)\n",
        "  RotMatrix = np.array(((cos, -sin), (sin, cos)))\n",
        "  image = image.dot(RotMatrix)\n",
        "  print(RotMatrix)\n",
        "  image = cv2.resize(image, (28,28))\n",
        "  return image\n",
        "\n",
        "def batch(self, images, labels, batch_size):\n",
        "  samples = images.shape[0]\n",
        "  batches = samples // batch_size\n",
        "  for i in range(batches): #np.zeros() if...\n",
        "    batch_images = np.array([[self.noise(self.rotate(image.reshape(28, 28)))] for image in images[i * batch_size:(i + 1) * batch_size]])\n",
        "    batch_labels = np.array(labels[i * batch_size:(i + 1) * batch_size])\n",
        "    return torch.tensor(batch_images), torch.tensor(batch_labels)"
      ],
      "metadata": {
        "id": "UBKFVCXTbGJT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv = nn.Conv2d(1, 16, kernel_size = (3, 3), stride = 1, padding = 1) # 1 сверточный слой с pooling 4x4 => 7x7 --> ????\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2) #28x28 --> 3x3 + ReLu() + stride(сдвиг kernel) = 2 + padding = 1 --> 2x2 Pooling\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size = (3, 3), stride = 1, padding = 1)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2) #14x14 --> 3x3 + ReLU() --> 2x2 Pooling\n",
        "    self.linear = nn.Linear(32 * 7 * 7, 256)\n",
        "    self.linear2 = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x_train):\n",
        "    output = nn.conv(x_train)\n",
        "    output = self.ReLu(output)\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = nn.conv2(output)\n",
        "    output = self.ReLu(output)\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = output.reshape(output.size(0), -1)\n",
        "\n",
        "    output = self.linear(output)\n",
        "    output = self.ReLu(output)\n",
        "\n",
        "    output = self.linear2(output)\n",
        "    output = self.Softmax(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def ReLu(x):\n",
        "    return (x >= 0) * x\n",
        "\n",
        "  def dReLu(x):\n",
        "    return x >= 0\n",
        "\n",
        "  def Softmax(x):\n",
        "    return np.exp(x)/sum(np.exp(x))\n",
        "\n",
        "  def dSoftmax(x):\n",
        "    return np.exp(x)/sum(np.exp(x)) * (1 - np.exp(x) / sum (np.exp(x)))\n",
        "\n",
        "#Someday...\n",
        "  #def backward(self, x_train, y_train):\n",
        "    #error = 2 * (output - y_train) / \n",
        "    #return output\n",
        "\n",
        "  #def Maxpool(x):\n",
        "    #return\n",
        "\n",
        "  #def dMaxpool(x):\n",
        "    #return"
      ],
      "metadata": {
        "id": "y7sUvFrziMub"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(cnn, epochs, optimizer, criterion, train_load):\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = 0.0\n",
        "    loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_load): #\n",
        "      output = cnn(images)\n",
        "      training_loss = criterion(output, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      training_loss.backward()\n",
        "      optimizer.step()\n",
        "      loss += training_loss\n",
        "      if i % 1000 == 0:\n",
        "        print(training_loss)\n",
        "        #print()\n",
        "  \n",
        "    print(\"Epoch \", epoch, \" loss is: \", loss)\n",
        "  print(\"What must be done is done!\")"
      ],
      "metadata": {
        "id": "7zPfNlJaPk-7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(cnn, test_load): #\n",
        "  cnn.eval()\n",
        "  correct_prediction = 0\n",
        "  true_positive = np.zeros(10, torch.int32)\n",
        "  false_positive = np.zeros(10, torch.int32)\n",
        "  false_negative = np.zeros(10, torch.int32)\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_load: #\n",
        "      output = cnn(input)\n",
        "      correct_prediction += (torch.max(output.data, 1) == labels).item()\n",
        "\n",
        "  accuracy = correct_prediction / labels.size(0)\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "  FScore = 2 * ((precision * recall) / (precision + recall))\n",
        "  print(( f\"Accuracy: {accuracy:.4f}\\n\"\n",
        "          f\"  Recall: {recall}\\n\"\n",
        "          f\"  Precision: {precision}\"\n",
        "          f\"  F-Score: {FScore:.4f}\"\n",
        "          ))"
      ],
      "metadata": {
        "id": "AuNurrejSqMH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #loader = Loader(path) #если в класс обернуть loader\n",
        "  #TODO: load() func update\n",
        "  #for key in (\"training_labels\", \"test_labels\"):\n",
        "    #train_load = (Xtrain, Ytrain) = loader.load()\n",
        "  #for key in (\"training_labels\", \"test_labels\"):\n",
        "    #test_load = (Xtest, Ytest) = loader.load()\n",
        "  cnn = CNN()\n",
        "  optimizer = optimize.Adam(cnn.parameters(), lr = 0.005)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #train(cnn, epochs, optimizer, criterion, train_load = (x_train, y_train))\n",
        "  #test(cnn, test_load = (x_test, y_test))"
      ],
      "metadata": {
        "id": "8mlJhTntrB6g"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}