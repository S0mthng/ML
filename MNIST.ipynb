{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR47YfD1uZM5"
      },
      "source": [
        "1. Скачайте датасет MNIST.\n",
        "2. Изучите выборку (какие там картинки, сколько картинок в каждом классе).\n",
        "3. Реализуйте загрузку датасета в память с диска с помощью OpenCV. Использовать готовые реализации датасетов и даталоудеров торча запрещено. Пока можно не заморачиваться многопоточностью.\n",
        "4. Реализуйте аугментацию данных: поворот картинки на определенное кол-во градусов (задается аргументом), зашумление картинки. Реализовывать на numpy. Использовать готовые решения тоже запрещено.\n",
        "5. Реализуйте способ скормить это нейросети (нормализация, приведение к одному разрешению).\n",
        "6. Напишите код нейросети. Использовать Sequential запрещено. Модель пишете руками, наследуясь от torch.nn.Module.\n",
        "7. Реализуйте цикл обучения нейросети. Использовать готовые трейнеры и model.fit() запрещено. Кол-во эпох, learning rate и прочие параметры задаются аргументами.\n",
        "8. Напишите код для инференса и оценки качества работы нейронной сети.\n",
        "9. Оценка качества должна заключаться в подсчете Accuracy, Recall, Precision, F-мера. Пользоваться готовыми решениями тоже запрещено.\n",
        "10. Оформите это как удобную библиотечку, которую можно склонить с гитхаба, положить куда надо файлы весов модели, выборку и обучить / инференсить модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z7HCFXFZCeY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import gzip\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFVsWvRyDacz"
      },
      "outputs": [],
      "source": [
        "path = \"./MNIST/archive/\"\n",
        "os.makedirs(path, exist_ok = True)\n",
        "\n",
        "source = {\"training_images\" : \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
        "          \"training_labels\" : \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
        "          \"test_images\" : \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
        "          \"test_labels\" : \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An3zwtAcDX0i"
      },
      "outputs": [],
      "source": [
        "data={}\n",
        "\n",
        "for key in source:\n",
        "  data[key] = source[key].split('/')[-1]\n",
        "  if os.path.exists(path + data[key]):\n",
        "    print(data[key] + \" was here long before you were born...\")\n",
        "  else:\n",
        "    urllib.request.urlretrieve(source[key], path + data[key])\n",
        "    print(data[key] + \" downloaded right now!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-jFh8MDnjA"
      },
      "outputs": [],
      "source": [
        "dataset = {}\n",
        "#path = \"/MNIST/archive\"\n",
        "\n",
        "# Images\n",
        "for key in (\"training_images\", \"test_images\"):\n",
        "  with gzip.open(os.path.join(path, data[key]), \"rb\") as mnist:\n",
        "    dataset[key] = np.frombuffer(mnist.read(), 'B', offset = 16).reshape(-1, 28 * 28)\n",
        "\n",
        "# Labels\n",
        "for key in (\"training_labels\", \"test_labels\"):\n",
        "    with gzip.open(os.path.join(path, data[key]), \"rb\") as mnist:\n",
        "      dataset[key] = np.frombuffer(mnist.read(), np.uint8, offset = 8)\n",
        "\n",
        "x_train, y_train, x_test, y_test = (\n",
        "    dataset[\"training_images\"],\n",
        "    dataset[\"training_labels\"],\n",
        "    dataset[\"test_images\"],\n",
        "    dataset[\"test_labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zob9T-OiDyYo",
        "outputId": "f67a4f17-96ab-4f7a-ad9a-7602b4f4a8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "5\n",
            "0\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "#cv2 check here\n",
        "\n",
        "#normalize\n",
        "training_sample, test_sample = 10000, 10000\n",
        "training_images = x_train[0:training_sample] / 255\n",
        "test_images = x_test[0:test_sample] / 255\n",
        "\n",
        "#toDimensions\n",
        "def trick(labels, dimension=10):\n",
        "    trick = labels[..., None] == np.arange(dimension)[None]\n",
        "    return trick.astype(np.float64)\n",
        "\n",
        "training_labels = trick(y_train[:training_sample])\n",
        "test_labels = trick(y_test[:test_sample])\n",
        "\n",
        "print(training_labels[0])\n",
        "print(training_labels[1])\n",
        "print(training_labels[2])\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train[1])\n",
        "print(y_train[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "1zEwdBO46j3t",
        "outputId": "7a77a44b-c4cf-43cd-d558-61cf1871f785"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c736b5c09024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlayer_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreLu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m#dropout_mask = gen.integers(0, 3, layer_1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#layer_1 *= dropout_mask * 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reLu' is not defined"
          ]
        }
      ],
      "source": [
        "seed = 123456\n",
        "gen = np.random.default_rng(seed)\n",
        "\n",
        "learning_rate = 0.005\n",
        "epochs = 100\n",
        "\n",
        "weights_1 = 0.15 * gen.random((784, 256)) - 0.01\n",
        "weights_2 = 0.15 * gen.random((256, 10)) - 0.01\n",
        "\n",
        "training_loss = []\n",
        "training_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "\n",
        "def reLu(x):\n",
        "    return (x >= 0) * x\n",
        "\n",
        "def reLuDiriv(x):\n",
        "    return x >= 0\n",
        "\n",
        "for j in range(epochs):\n",
        "  loss = 0.0\n",
        "  accuracy = 0\n",
        "\n",
        "  for i in range(len(training_images)):\n",
        "        layer_0 = training_images[i]\n",
        "        layer_1 = np.dot(layer_0, weights_1)\n",
        "        layer_1 = reLu(layer_1)\n",
        "        #dropout_mask = gen.integers(0, 3, layer_1.shape)\n",
        "        #layer_1 *= dropout_mask * 3\n",
        "        layer_2 = np.dot(layer_1, weights_2)\n",
        "        #TODO: softmax func\n",
        "\n",
        "        loss += np.sum((training_labels[i] - layer_2) ** 2)\n",
        "        accuracy += int(np.argmax(layer_2) == np.argmax(training_labels[i]))\n",
        "        #layer_1_delta *= dropout_mask\n",
        "\n",
        "  training_loss.append(loss)\n",
        "  training_accuracy.append(accuracy)\n",
        "\n",
        "  result = reLu(test_images @ weights_1) @ weights_2\n",
        "\n",
        "  tloss = np.sum((test_labels - result) ** 2)\n",
        "  taccuracy = np.sum(np.argmax(result, 1) == np.argmax(test_labels, 1))\n",
        "\n",
        "  test_loss.append(tloss)\n",
        "  test_accuracy.append(taccuracy)\n",
        "\n",
        "  print(( f\"Epoch: {j}\\n\"\n",
        "          f\"  Training set loss: {loss / len(training_images):.3f}\\n\"\n",
        "          f\"  Training set accuracy: {accuracy / len(training_images)}\\n\"\n",
        "          f\"  Test set loss: {tloss / len(test_images):.3f}\\n\"\n",
        "          f\"  Test set accuracy: {taccuracy / len(test_images)}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
